{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RickeyEstes2/Public/blob/main/WikiSummaryComplete.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm really enjoying this course on Udemy and think you might like it too.\n",
        "https://www.udemy.com/share/104xGa3@Ll9jl5e5GcVJttkFyKapOxGfFgry6HrS_Dfmh8G6HEdqX2xXzdQJrcEW0Yuvb9T4ew==/"
      ],
      "metadata": {
        "id": "JqT3z3tg0TTf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##<font color='#ffhh33'>Section 18\n",
        "###<font color='#aaaaaa'> -<b> Extraction-Based Summarization</b>"
      ],
      "metadata": {
        "id": "CA7VhQLY0Qzv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ob6Jc07nKjZ"
      },
      "source": [
        "##<font color=\"#fd79a8\">Extraction-Based Summarizer <br><font color=\"#48dbfb\">Scraped Wikipedia articles using Beautiful Soup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L80gVJiZ1fvJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "743b2f3c-e3ab-4a05-d43b-f2cd6ce54ada"
      },
      "source": [
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import sys\n",
        "import csv\n",
        "\n",
        "#persian cuisine\n",
        "#scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Iranian_cuisine')\n",
        "\n",
        "\n",
        "scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Brain-computer_interface')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "article = scraped_data.read()\n",
        "\n",
        "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
        "\n",
        "paragraphs = parsed_article.find_all('p')\n",
        "\n",
        "article_text = \"\"\n",
        "\n",
        "for p in paragraphs:\n",
        "    article_text += p.text"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_4EmP8l1tLi"
      },
      "source": [
        "# Removing Square Brackets and Extra Spaces\n",
        "article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
        "#any whitespace character \\s+\n",
        "article_text = re.sub(r'\\s+', ' ', article_text)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-AKwDkc15h6"
      },
      "source": [
        "# Removing special characters and digits\n",
        "formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )\n",
        "#any whitespace character \\s+\n",
        "formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3dlAgyRvLa4"
      },
      "source": [
        "##<font color=\"#fd79a8\">Convert paragraphs to sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1BxJpQx15ss"
      },
      "source": [
        "sentence_list = nltk.sent_tokenize(article_text)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGWRc5NY2g8J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d256ad36-66f3-452b-ba3a-4adec65ed566"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ-LlCwBwE7J"
      },
      "source": [
        "###<font color=\"#fd79a8\"> Loop to calculate the word frequencies. <br>Tokenize the sentences<br>if word is not a stopword and is in the word list, the count is added"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA4mWOgK2X6Y"
      },
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "word_frequencies = {}\n",
        "for word in nltk.word_tokenize(formatted_article_text):\n",
        "    if word not in stopwords:\n",
        "        if word not in word_frequencies.keys():\n",
        "            word_frequencies[word] = 1\n",
        "        else:\n",
        "            word_frequencies[word] += 1"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U4v06sl2xA_v"
      },
      "source": [
        "###<font color=\"#48dbfb\"> Keys() method<br>The keys() method returns a view object. The view object contains the keys of the dictionary, as a list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u70SSN1cxO2i",
        "outputId": "1d871249-60c2-4b2a-8940-8341d780bbdc"
      },
      "source": [
        "shoe = {\n",
        "  \"brand\": \"Nike\",\n",
        "  \"series\": \"Air Max\",\n",
        "  \"price\": 100\n",
        "}\n",
        "\n",
        "var = shoe.keys()\n",
        "\n",
        "print(var)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['brand', 'series', 'price'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOPnTZ9ixUvl"
      },
      "source": [
        "###<font color=\"#48dbfb\">When an item is added in the dictionary, the view object also gets updated:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5rYePFbxSd0",
        "outputId": "3fe188ef-5444-43b2-a90d-badfa45a4ff1"
      },
      "source": [
        "shoe[\"color\"] = \"red\"\n",
        "\n",
        "print(var)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['brand', 'series', 'price', 'color'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzSatUTS8WK7"
      },
      "source": [
        "###<font color=\"#48dbfb\">Find weighted frequency of occurence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D34gqm-42lBq"
      },
      "source": [
        "maximum_frequncy = max(word_frequencies.values())\n",
        "\n",
        "for word in word_frequencies.keys():\n",
        "    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xw5PmlZHycfs",
        "outputId": "09f8597e-f4ea-470d-91b9-467792a50149"
      },
      "source": [
        "shoe = {\n",
        "  \"brand\": \"Nike\",\n",
        "  \"series\": \"Air Max\",\n",
        "  \"price\": 100\n",
        "}\n",
        "\n",
        "var = shoe.values()\n",
        "\n",
        "print(var)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_values(['Nike', 'Air Max', 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2LMj2pa8L0k"
      },
      "source": [
        "###<font color=\"#48dbfb\">Replace words with weighted frequency in sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNUVxbcD2lOD"
      },
      "source": [
        "sentence_scores = {}\n",
        "for sent in sentence_list:\n",
        "    for word in nltk.word_tokenize(sent.lower()):\n",
        "        if word in word_frequencies.keys():\n",
        "            if len(sent.split(' ')) < 30:\n",
        "                if sent not in sentence_scores.keys():\n",
        "                    sentence_scores[sent] = word_frequencies[word]\n",
        "                else:\n",
        "                    sentence_scores[sent] += word_frequencies[word]"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0mMnhCX8lLJ"
      },
      "source": [
        "####<font color=\"#fd79a8\">Heap queue <br>heap queue algorithm, also known as the priority queue algorithm<br>It makes it possible to view the data (words/scores) -  our heap, as a regular Python list<br><font color=\"#0abde3\">heapq.nlargest(n, iterable, key=None)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThhA39562rxN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "6c96b0ea-ce8e-4da0-eb32-0fc72cdf06ec"
      },
      "source": [
        "import heapq\n",
        "summary_sentences = heapq.nlargest(10, sentence_scores, key=sentence_scores.get)\n",
        "\n",
        "summary = ' '.join(summary_sentences)\n",
        "summary"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Several groups have been able to capture complex brain motor cortex signals by recording from neural ensembles (groups of neurons) and using these to control external devices. Due to the cortical plasticity of the brain, signals from implanted prostheses can, after adaptation, be handled by the brain like natural sensor or effector channels. Their BCI used high-density electrocorticography to tap neural activity from a patient's brain and used deep learning methods to synthesize speech. Implementations of BCIs range from non-invasive (EEG, MEG, EOG, MRI) and partially invasive (ECoG and endovascular) to invasive (microelectrode array), based on how close electrodes get to brain tissue. As well as enabling Jatich to control a computer cursor the signals were also used to drive the nerve controllers embedded in his hands, restoring some movement. The monkey was brain controlling the position of an avatar arm while receiving sensory feedback through direct intracortical stimulation (ICMS) in the arm representation area of the sensory cortex. Flexible neural interfaces have been extensively tested in recent years in an effort to minimize brain tissue trauma related to mechanical mismatch between electrode and tissue. 1988, 1990), control of a physical object, a robot, using a brain rhythm (alpha) (Bozinovski et al. However, studies have also used fMRI to study different changes in the brain as persons undergo BCI-based stroke rehab training. In the 1960s a researcher was successful after some training in using EEG to create Morse code using their brain alpha waves.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "with open('example7777.txt', 'w') as f:\n",
        "  f.write(summary)\n",
        "\n",
        "files.download('example7777.txt')"
      ],
      "metadata": {
        "id": "jvxmLdMQinVm",
        "outputId": "1890e1f0-744d-4137-d6b6-4e3c4f58dc13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_016688a1-5c14-478d-a3f8-852798e24328\", \"example7777.txt\", 1587)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}